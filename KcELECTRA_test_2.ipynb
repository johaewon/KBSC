{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcRwkahjmv+oaY4cLJKTjM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johaewon/KBSC/blob/main/KcELECTRA_test_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8ToDlUKHENk"
      },
      "outputs": [],
      "source": [
        "pip install soynlp emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "EYZR_pUyIo0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fVsZk0IPIo26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW \n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "2cDbuy6hIo5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "qoReLTWgIo7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_excel('/content/drive/MyDrive/한국어_단발성_대화_데이터셋.xlsx')"
      ],
      "metadata": {
        "id": "DlvW2CaIIo9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.loc[(test_data['Emotion'] == \"공포\"), 'Emotion'] = 0 \n",
        "test_data.loc[(test_data['Emotion'] == \"놀람\"), 'Emotion'] = 1  \n",
        "test_data.loc[(test_data['Emotion'] == \"분노\"), 'Emotion'] = 2  \n",
        "test_data.loc[(test_data['Emotion'] == \"슬픔\"), 'Emotion'] = 3  \n",
        "test_data.loc[(test_data['Emotion'] == \"중립\"), 'Emotion'] = 4  \n",
        "test_data.loc[(test_data['Emotion'] == \"행복\"), 'Emotion'] = 5  \n",
        "test_data.loc[(test_data['Emotion'] == \"혐오\"), 'Emotion'] = 6"
      ],
      "metadata": {
        "id": "AYaZzWf9Io_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = []\n",
        "for sen, label in zip(test_data['Sentence'], test_data['Emotion']):\n",
        "  data_train = []\n",
        "  data_train.append(sen)\n",
        "  data_train.append(str(label))\n",
        "\n",
        "  train_dataset.append(data_train)"
      ],
      "metadata": {
        "id": "Vn1pAaZOIpBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDataset(Dataset):\n",
        "  \n",
        "  def __init__(self, dataset):\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "\n",
        "    self.sentences = [str([i[0]]) for i in dataset]\n",
        "    self.labels = [np.int32(i[1]) for i in dataset]\n",
        "\n",
        "  def __len__(self):\n",
        "    return (len(self.labels))\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    text = self.sentences[i]\n",
        "    y = self.labels[i]\n",
        "\n",
        "    inputs = self.tokenizer(\n",
        "        text, \n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        max_length=64,\n",
        "        pad_to_max_length=True,\n",
        "        add_special_tokens=True\n",
        "        )\n",
        "    \n",
        "    input_ids = inputs['input_ids'][0]\n",
        "    attention_mask = inputs['attention_mask'][0]\n",
        "\n",
        "    return input_ids, attention_mask, y"
      ],
      "metadata": {
        "id": "H8RnXjUcJIIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TrainDataset(train_dataset)"
      ],
      "metadata": {
        "id": "ajlo2-1EJIKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "model = AutoModel.from_pretrained(\"beomi/KcELECTRA-base\", num_labels=363)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "9UnEtlsSJINb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji==1.7"
      ],
      "metadata": {
        "id": "gkEJrSN5L1Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import emoji\n",
        "from soynlp.normalizer import repeat_normalize\n",
        "\n",
        "emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n",
        "pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-ㅣ가-힣{emojis}]+')\n",
        "url_pattern = re.compile(\n",
        "    r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n",
        "\n",
        "def clean(x):\n",
        "    x = pattern.sub(' ', x)\n",
        "    x = url_pattern.sub('', x)\n",
        "    x = x.strip()\n",
        "    x = repeat_normalize(x, num_repeats=2)\n",
        "    return x"
      ],
      "metadata": {
        "id": "VTMB58v1JIP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 2"
      ],
      "metadata": {
        "id": "X4LxBRKVMh4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=5, shuffle=True)"
      ],
      "metadata": {
        "id": "v4ot8RQtMklA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "#정확도 측정을 위한 함수 정의\n",
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for i in range(epochs):\n",
        "  train_acc = 0.0\n",
        "  total_loss = 0.0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  batches = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for input_ids_batch, attention_masks_batch, y_batch in tqdm(train_dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    y_batch = y_batch.long().to(device)\n",
        "    y_pred = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
        "    y_pred = y_pred[:, -1, :]\n",
        "    loss = loss_fn(y_pred, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    train_acc += calc_accuracy(y_pred, y_batch)\n",
        "    total += len(y_batch)\n",
        "\n",
        "    batches += 1\n",
        "    if batches % 50 == 0:\n",
        "      print(\"epoch {} loss {} train acc {}\".format(i+1, loss.data.cpu().numpy(), train_acc / (batches+1)))\n",
        "  print(\"epoch {} loss {} train acc {}\".format(i+1, loss.data.cpu().numpy(), train_acc / (batches+1)))\n",
        "  model.eval()"
      ],
      "metadata": {
        "id": "U22gzN-RMkn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/model.pt\")"
      ],
      "metadata": {
        "id": "7JohuKQMMwpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(sentence):\n",
        "    data = [sentence, '0']\n",
        "    dataset_another = [data]\n",
        "    logits = 0\n",
        "    another_test = TrainDataset(dataset_another)\n",
        "    test_dataloader = torch.utils.data.DataLoader(another_test)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for input_ids_batch, attention_masks_batch, y_batch in test_dataloader:\n",
        "        y_batch = y_batch.long().to(device)\n",
        "        out = model(input_ids_batch.to(device), attention_mask=attention_masks_batch.to(device))[0]\n",
        "        out = out[:, -1, :]\n",
        "\n",
        "        for i in out:\n",
        "            logits = i\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            logits = np.argmax(logits)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "CuEltT2bMw1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(\"\")"
      ],
      "metadata": {
        "id": "JuUM06bvMw9c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}